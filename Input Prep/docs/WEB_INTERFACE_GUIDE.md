# Web Interface Guide

## üåê Accessing the Web Interface

Once the server is running, open your browser and navigate to:

```
http://localhost:8000
```

You'll see a beautiful, modern interface for interacting with the LLM-Protect system.

## üöÄ Quick Start

### 1. Start the Server

```bash
# Install dependencies (first time only)
pip install -r requirements.txt

# Start the server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### 2. Open Web Interface

Navigate to `http://localhost:8000` in your browser.

### 3. Enter Your Prompt

Type your question or prompt in the text area. For example:
```
What is machine learning and how does it work?
```

### 4. (Optional) Upload a File

Click "Choose File" and select a supported file:
- `.txt` - Plain text
- `.md` - Markdown
- `.pdf` - PDF documents
- `.docx` - Word documents

The system will extract text from the file and include it in the processing.

### 5. (Optional) Add External Data

If you have additional context or RAG data, enter it as a JSON array:
```json
["Machine learning is a subset of AI", "It uses algorithms to learn from data"]
```

### 6. Click "Process & Generate Response"

The system will:
1. **Prepare** your input (parse, extract, normalize, sign with HMAC)
2. **Generate** a response using Gemma 2B

You'll see real-time logs showing each step!

## üìä Understanding the Interface

### Left Panel: User Input
- **Your Prompt**: Main input text area
- **Upload File**: Optional file upload button
- **External Data**: Optional RAG/context data
- **Vector DB Checkbox**: Enable vector database retrieval (if configured)
- **Process Button**: Starts the processing pipeline

### Right Panel: Model Information
- Shows the model being used (Gemma 2B)
- Lists the 7-step processing pipeline
- Displays security features

### Processing Log (Bottom)
Real-time log showing:
- Step-by-step progress
- Request ID for tracking
- Token counts and timing
- Success/error messages

Color coding:
- üîµ Blue: Info messages
- üü¢ Green: Success messages
- üî¥ Red: Error messages
- üü° Yellow: Warning messages

### Results Panel (Appears after generation)
- **Generated Response**: The LLM's output
- **Statistics**:
  - Input Tokens: Tokens in your prompt
  - Output Tokens: Tokens generated by LLM
  - Total Time: Complete processing time
  - RAG Chunks: Number of external data chunks

## üìù Example Usage Scenarios

### Scenario 1: Simple Question

**Input:**
```
Explain quantum computing in simple terms
```

**What Happens:**
1. Text is normalized and tokenized
2. No external data (RAG disabled)
3. Sent to Gemma 2B for generation
4. Response displayed with timing stats

**Expected Log:**
```
[10:00:00] Starting input preparation pipeline...
[10:00:00] User prompt length: 44 characters
[10:00:00] Step 1: Preparing input with HMAC signatures...
[10:00:00] ‚úì Input preparation complete
[10:00:00]   - Request ID: 550e8400...
[10:00:00]   - Tokens: 11
[10:00:00]   - External chunks: 0
[10:00:00]   - Prep time: 15.32ms
[10:00:00] Step 2: Generating LLM response...
[10:00:02] ‚úì LLM response generated successfully
[10:00:02]   - Input tokens: 11
[10:00:02]   - Output tokens: 150
[10:00:02]   - Generation time: 2340.50ms
[10:00:02] ‚úì Process complete!
```

### Scenario 2: Question with File Upload

**Input:**
- Prompt: "Summarize this document"
- File: `research_paper.pdf` (10 pages)

**What Happens:**
1. PDF text extracted and chunked
2. Each chunk signed with HMAC
3. Chunks added as external context
4. Combined with user prompt
5. Sent to Gemma 2B
6. Response includes information from PDF

**Expected Log:**
```
[10:05:00] Starting input preparation pipeline...
[10:05:00] User prompt length: 23 characters
[10:05:00] File uploaded: research_paper.pdf (245678 bytes)
[10:05:00] Step 1: Preparing input with HMAC signatures...
[10:05:01] ‚úì Input preparation complete
[10:05:01]   - Request ID: 661f9511...
[10:05:01]   - Tokens: 450
[10:05:01]   - External chunks: 8
[10:05:01]   - Prep time: 68.45ms
[10:05:01] Step 2: Generating LLM response...
[10:05:04] ‚úì LLM response generated successfully
[10:05:04]   - Input tokens: 450
[10:05:04]   - Output tokens: 200
[10:05:04]   - Generation time: 3125.80ms
[10:05:04] ‚úì Process complete!
```

### Scenario 3: Question with External Context (RAG)

**Input:**
- Prompt: "What's the weather forecast?"
- External Data: `["Sunny today", "Temperature 25¬∞C", "No rain expected"]`

**What Happens:**
1. User prompt normalized
2. External data wrapped with `[EXTERNAL]` delimiters
3. Each external chunk signed with HMAC
4. Combined for LLM
5. Response uses provided context

**Expected Log:**
```
[10:10:00] Starting input preparation pipeline...
[10:10:00] User prompt length: 27 characters
[10:10:00] Step 1: Preparing input with HMAC signatures...
[10:10:00] ‚úì Input preparation complete
[10:10:00]   - Request ID: 772fa622...
[10:10:00]   - Tokens: 35
[10:10:00]   - External chunks: 3
[10:10:00]   - Prep time: 22.10ms
[10:10:00] Step 2: Generating LLM response...
[10:10:02] ‚úì LLM response generated successfully
[10:10:02]   - Input tokens: 35
[10:10:02]   - Output tokens: 45
[10:10:02]   - Generation time: 1850.30ms
[10:10:02] ‚úì Process complete!
```

### Scenario 4: Question with Emojis

**Input:**
```
Tell me about space travel üöÄüåå‚ú®
```

**What Happens:**
1. Text normalized (emojis preserved)
2. Emojis extracted separately
3. Both stored in output
4. LLM receives full text with emojis
5. Emoji metadata tracked

**Note**: Emojis are preserved in BOTH the normalized text AND extracted separately for analysis!

## üé® UI Features

### Real-time Feedback
- Processing button shows "‚è≥ Processing..." during operation
- Button disabled to prevent duplicate submissions
- Auto-scrolling log panel

### Error Handling
- Clear error messages in red
- Detailed error information in logs
- Graceful degradation on failures

### Mobile Responsive
- Adapts to smaller screens
- Stacked layout on mobile devices
- Touch-friendly buttons

## üîß Troubleshooting

### "Model not loading" Error

**Problem**: Gemma 2B model fails to load

**Solutions**:
1. Check if you have enough RAM (4GB+ recommended)
2. Install transformers: `pip install transformers torch`
3. Check internet connection for first-time model download
4. Try CPU mode if GPU issues: Model automatically detects available device

### "File upload failed" Error

**Problem**: File upload rejected

**Solutions**:
1. Check file size (<10MB)
2. Verify file type (TXT/MD/PDF/DOCX only)
3. Ensure file is not corrupted
4. Try a different file

### "Processing taking too long"

**Problem**: Generation seems stuck

**Expected Times**:
- Simple text: 2-5 seconds
- With file (PDF): 5-10 seconds
- Large context: 10-20 seconds

**Note**: First request may take longer as model loads (30-60 seconds)

### "Connection refused" Error

**Problem**: Can't reach `http://localhost:8000`

**Solutions**:
1. Ensure server is running: `uvicorn app.main:app --reload`
2. Check port 8000 is not in use
3. Try different port: `uvicorn app.main:app --port 8001`
4. Check firewall settings

## üì± Using on Mobile

The interface is mobile-responsive:

1. Open browser on your phone/tablet
2. Navigate to your computer's IP address:
   ```
   http://192.168.1.X:8000
   ```
   (Replace X with your computer's local IP)
3. Use the interface as normal

**Note**: Ensure both devices are on the same network.

## üîó API Integration

The web interface uses these API endpoints:

1. **POST /api/v1/prepare-text**
   - Prepares and validates input
   - Returns PreparedInput object

2. **POST /api/v1/generate**
   - Takes PreparedInput
   - Returns generated response

You can integrate these endpoints into your own applications!

## üìä What the Logs Tell You

### Request ID
```
Request ID: 550e8400-e29b-41d4-a716-446655440000
```
- Unique identifier for tracking this request
- Used in all logs for correlation
- Included in error reports

### Tokens
```
Tokens: 38
```
- Estimated token count for input
- Used for rate limiting and cost estimation
- Affects generation time

### External Chunks
```
External chunks: 2
```
- Number of RAG/external data pieces
- Each chunk has an HMAC signature
- More chunks = more context for LLM

### Prep Time
```
Prep time: 45.20ms
```
- Time for input preparation
- Includes: parsing, extraction, normalization, signing
- Should be <100ms for most requests

### Generation Time
```
Generation time: 2340.50ms
```
- Time for LLM to generate response
- Varies based on:
  - Input length
  - Output length (max_new_tokens)
  - Hardware (CPU vs GPU)

## üéØ Best Practices

### For Best Results:
1. **Be specific** in your prompts
2. **Provide context** via external data or file uploads
3. **Keep files small** (<5MB) for faster processing
4. **Monitor logs** to understand what's happening
5. **Check request ID** if reporting issues

### For Best Performance:
1. **Use GPU** if available (automatically detected)
2. **Reduce max_new_tokens** for faster responses
3. **Avoid huge PDFs** (split into smaller files)
4. **Batch similar requests** when possible
5. **Cache model** (loads once, stays in memory)

## üöÄ Advanced Features

### Custom Generation Parameters

The web interface uses defaults, but you can customize via API:

```python
import requests

prepared = # ... get from /prepare-text

response = requests.post('http://localhost:8000/api/v1/generate', json={
    'prepared_input': prepared,
    'max_new_tokens': 1024,  # More tokens
    'temperature': 0.9,       # More creative
    'top_p': 0.95,            # Nucleus sampling
    'do_sample': True         # Use sampling
})
```

### Monitoring Multiple Requests

Each request gets a unique ID. Track them in logs:

```bash
# Filter logs by request ID
grep "550e8400" server.log

# Count requests per minute
grep "Starting request" server.log | wc -l
```

### Integration with Layer 0

The PreparedInput output is ready for Layer 0 (heuristics):

```python
prepared = response.json()  # from /prepare-text

# Layer 0 can now:
- Check for regex patterns
- Detect separator attacks
- Validate HMAC signatures
- Analyze token ratios
```

## üìö Related Documentation

- [OUTPUT_FORMATS.md](OUTPUT_FORMATS.md) - Detailed output formats
- [USAGE.md](USAGE.md) - API usage guide
- [README.md](README.md) - Project overview
- [QUICKSTART.md](QUICKSTART.md) - 5-minute setup

## üÜò Getting Help

If you encounter issues:

1. Check the **Processing Log** for error messages
2. Note the **Request ID** from logs
3. Review [OUTPUT_FORMATS.md](OUTPUT_FORMATS.md) for expected formats
4. Check API status at `/health`
5. View API docs at `/docs`

## ‚ú® Summary

The web interface provides:
- ‚úÖ Easy file uploads with drag-and-drop
- ‚úÖ Real-time processing logs
- ‚úÖ Beautiful, modern UI
- ‚úÖ Complete statistics and timing
- ‚úÖ Mobile responsive design
- ‚úÖ Error handling and feedback
- ‚úÖ Direct LLM integration

Try it now at `http://localhost:8000`!

